{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e497a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukasz.wrobel\\AppData\\Local\\Temp\\ipykernel_28212\\3549646536.py:55: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kolumn num: 11, kat: 3\n",
      "Najlepsze parametry: (6, 200, 0.0) AUC_mean= 0.694\n",
      "Średnie metryki CV (drzewo):\n",
      " AUC        0.6940\n",
      "PR_AUC     0.3674\n",
      "KS         0.2867\n",
      "Brier      0.2262\n",
      "LogLoss    0.6418\n",
      "ECE        0.2510\n",
      "dtype: float64\n",
      "\n",
      "Metryki OOT (drzewo):\n",
      " AUC        0.6877\n",
      "PR_AUC     0.4282\n",
      "KS         0.2830\n",
      "Brier      0.1858\n",
      "LogLoss    0.5512\n",
      "ECE        0.0087\n",
      "dtype: float64\n",
      "\n",
      "Artefakty zapisano w: c:\\Users\\lukasz.wrobel\\Desktop\\PRACA MAGISTERSKA\\pliki\\artifacts\\artifacts_44_tree\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Rozdz. 4.4 — Drzewo decyzyjne (CART) – pełny pipeline uczący\n",
    "# ===========================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, roc_curve\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# ---------- ścieżki / artefakty ----------\n",
    "ART = \"artifacts_44_tree\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Wskaźniki zysku (dostosuj do swoich realiów)\n",
    "PROFIT_GOOD = 1_000\n",
    "LOSS_BAD   = -5_000\n",
    "\n",
    "# liczba podziałów w walidacji czasowej\n",
    "N_SPLITS_TIME = 6\n",
    "N_BINS_CALIB  = 10\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "# ---------- 1) dane ----------\n",
    "SNAP_PATH = Path(\"C:/Users/lukasz.wrobel/Desktop/PRACA MAGISTERSKA/pliki/artifacts/artifacts/engineered_snapshot.csv\")\n",
    "if not SNAP_PATH.exists():\n",
    "    SNAP_PATH = Path(\"engineered_snapshot.csv\")\n",
    "\n",
    "df = pd.read_csv(SNAP_PATH)\n",
    "if \"issue_d\" in df.columns:\n",
    "    df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "\n",
    "assert \"loan_status_bin\" in df.columns, \"Brak kolumny 'loan_status_bin' w snapshotcie.\"\n",
    "\n",
    "# docelowa y\n",
    "df[\"loan_status_bin\"] = pd.to_numeric(df[\"loan_status_bin\"], errors=\"coerce\")\n",
    "df = df.loc[df[\"loan_status_bin\"].isin([0,1])].copy()\n",
    "y = df[\"loan_status_bin\"].astype(\"int8\").to_numpy()\n",
    "\n",
    "# usuń ±inf -> NaN (imputer je obsłuży)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# zdefiniuj listy cech dynamicznie (bez kolumn datetime)\n",
    "feature_cols = [c for c in df.columns if c != \"loan_status_bin\" and not np.issubdtype(df[c].dtype, np.datetime64)]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n",
    "print(f\"#kolumn num: {len(num_cols)}, kat: {len(cat_cols)}\")\n",
    "\n",
    "# ---------- 2) helpery ----------\n",
    "def time_blocks(frame: pd.DataFrame, date_col=\"issue_d\", n_splits=N_SPLITS_TIME):\n",
    "    \"\"\"Zwraca listę (train_idx, valid_idx) wzrastających bloków czasowych po miesiącach.\"\"\"\n",
    "    if date_col not in frame.columns or frame[date_col].isna().all():\n",
    "        # fallback: jeden fold 80/20 bez czasu\n",
    "        n = len(frame)\n",
    "        cut = int(n*0.8)\n",
    "        idx = frame.index.to_numpy()\n",
    "        return [(idx[:cut], idx[cut:])]\n",
    "    months = frame[date_col].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    if len(uniq) < n_splits:\n",
    "        n_splits = max(2, len(uniq))\n",
    "    chunks = np.array_split(uniq, n_splits)\n",
    "    pairs = []\n",
    "    for i in range(1, len(chunks)):\n",
    "        tr_m = np.concatenate(chunks[:i])\n",
    "        va_m = chunks[i]\n",
    "        tr_idx = frame.index[months.isin(tr_m)]\n",
    "        va_idx = frame.index[months.isin(va_m)]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            pairs.append((tr_idx, va_idx))\n",
    "    return pairs\n",
    "\n",
    "def ks_score(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=20):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        m = (idx == b)\n",
    "        if m.sum()==0: \n",
    "            continue\n",
    "        ece += m.mean() * abs(y_prob[m].mean() - y_true[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def decile_table(y_true, y_prob, deciles=10):\n",
    "    d = pd.DataFrame({\"y\": y_true, \"p\": y_prob}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    d[\"decile\"] = pd.qcut(d.index, q=deciles, labels=False) + 1\n",
    "    tab = d.groupby(\"decile\").agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        bad=(\"y\",\"sum\"),\n",
    "        good=(\"y\", lambda s: (1-s).sum()),\n",
    "        prob_mean=(\"p\",\"mean\")\n",
    "    ).reset_index()\n",
    "    tab[\"bad_rate\"] = tab[\"bad\"]/tab[\"n\"]\n",
    "    total_bad, total_good = tab[\"bad\"].sum(), tab[\"good\"].sum()\n",
    "    tab[\"cum_bad\"]  = tab[\"bad\"].cumsum()/max(total_bad,1)\n",
    "    tab[\"cum_good\"] = tab[\"good\"].cumsum()/max(total_good,1)\n",
    "    tab[\"ks\"] = (tab[\"cum_bad\"] - tab[\"cum_good\"]).abs()\n",
    "    return tab\n",
    "\n",
    "def profit_curve(y_true, y_prob, profit_good=PROFIT_GOOD, loss_bad=LOSS_BAD, steps=201):\n",
    "    taus = np.linspace(0,1,steps)\n",
    "    ev = []\n",
    "    for t in taus:\n",
    "        acc = y_prob < t\n",
    "        tg = ((y_true==0) & acc).sum()\n",
    "        tb = ((y_true==1) & acc).sum()\n",
    "        ev.append(tg*profit_good + tb*loss_bad)\n",
    "    return taus, np.array(ev)\n",
    "\n",
    "# ---------- 3) preprocessing ----------\n",
    "# Drzewo nie wymaga standaryzacji; ale robimy imputację i OHE\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True))  # dodaje flagi braków\n",
    "    # brak skalowania – niepotrzebne dla drzewa\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols),\n",
    "     (\"cat\", cat_pipe, cat_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# bazowy klasyfikator CART\n",
    "tree_base = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=100,   # sensowny próg dla dużych danych\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "tree_pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", tree_base)\n",
    "])\n",
    "\n",
    "# ---------- 4) walidacja czasowa + prosty tuning ----------\n",
    "param_grid = {\n",
    "    \"clf__max_depth\": [6, 8, 10, 12, None],\n",
    "    \"clf__min_samples_leaf\": [50, 100, 200],\n",
    "    \"clf__ccp_alpha\": [0.0, 1e-4, 5e-4, 1e-3]\n",
    "}\n",
    "\n",
    "folds = time_blocks(df, \"issue_d\", n_splits=N_SPLITS_TIME)\n",
    "results, last = [], {}\n",
    "best_auc, best_params = -np.inf, None\n",
    "\n",
    "for max_depth in param_grid[\"clf__max_depth\"]:\n",
    "    for min_leaf in param_grid[\"clf__min_samples_leaf\"]:\n",
    "        for ccp in param_grid[\"clf__ccp_alpha\"]:\n",
    "            tree_pipe.set_params(clf__max_depth=max_depth,\n",
    "                                 clf__min_samples_leaf=min_leaf,\n",
    "                                 clf__ccp_alpha=ccp)\n",
    "            fold_metrics = []\n",
    "            y = y.astype(\"int64\") if isinstance(y, pd.Series) else pd.Series(y, index=df.index)\n",
    "            for tr_idx, va_idx in folds:\n",
    "                Xtr, ytr = df.loc[tr_idx, :], y[tr_idx]\n",
    "                Xva, yva = df.loc[va_idx, :], y[va_idx]\n",
    "                tree_pipe.fit(Xtr, ytr)\n",
    "                p = tree_pipe.predict_proba(Xva)[:,1]\n",
    "                fold_metrics.append(roc_auc_score(yva, p))\n",
    "                last = {\"Xtr\":Xtr, \"ytr\":ytr, \"Xva\":Xva, \"yva\":yva, \"pva\":p}\n",
    "            mean_auc = float(np.mean(fold_metrics))\n",
    "            results.append({\"max_depth\":max_depth, \"min_leaf\":min_leaf, \"ccp_alpha\":ccp, \"AUC_mean\":mean_auc})\n",
    "            if mean_auc > best_auc:\n",
    "                best_auc, best_params = mean_auc, (max_depth, min_leaf, ccp)\n",
    "\n",
    "cv_grid = pd.DataFrame(results).sort_values(\"AUC_mean\", ascending=False)\n",
    "cv_grid.to_csv(f\"{ART}/cv_grid_tree.csv\", index=False)\n",
    "print(\"Najlepsze parametry:\", best_params, \"AUC_mean=\", round(best_auc,4))\n",
    "\n",
    "# ustaw najlepsze i policz pełny zestaw metryk na foldach\n",
    "tree_pipe.set_params(clf__max_depth=best_params[0],\n",
    "                     clf__min_samples_leaf=best_params[1],\n",
    "                     clf__ccp_alpha=best_params[2])\n",
    "\n",
    "metrics = []\n",
    "for tr_idx, va_idx in folds:\n",
    "    Xtr, ytr = df.loc[tr_idx, :], y[tr_idx]\n",
    "    Xva, yva = df.loc[va_idx, :], y[va_idx]\n",
    "    tree_pipe.fit(Xtr, ytr)\n",
    "    p = tree_pipe.predict_proba(Xva)[:,1]\n",
    "    metrics.append({\n",
    "        \"AUC\": roc_auc_score(yva, p),\n",
    "        \"PR_AUC\": average_precision_score(yva, p),\n",
    "        \"KS\": ks_score(yva, p),\n",
    "        \"Brier\": brier_score_loss(yva, p),\n",
    "        \"LogLoss\": log_loss(yva, p, labels=[0,1]),\n",
    "        \"ECE\": ece_score(yva, p)\n",
    "    })\n",
    "    last = {\"Xtr\":Xtr, \"ytr\":ytr, \"Xva\":Xva, \"yva\":yva, \"pva\":p}\n",
    "\n",
    "cv_results = pd.DataFrame(metrics)\n",
    "cv_results.to_csv(f\"{ART}/cv_fold_metrics_tree.csv\", index=False)\n",
    "cv_mean = cv_results.mean()\n",
    "cv_mean.to_csv(f\"{ART}/cv_metrics_mean_tree.csv\", header=False)\n",
    "print(\"Średnie metryki CV (drzewo):\\n\", cv_mean.round(4))\n",
    "\n",
    "# ---------- 5) ROC i kalibracja (ostatni fold) ----------\n",
    "fpr, tpr, _ = roc_curve(last[\"yva\"], last[\"pva\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(last['yva'],last['pva']):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — Decision Tree (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_last_fold_tree.png\", dpi=160); plt.close()\n",
    "\n",
    "frac_pos, mean_pred = calibration_curve(last[\"yva\"], last[\"pva\"], n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja — Decision Tree (ostatni fold)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/calibration_last_fold_tree.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 6) Krzywa zysku i próg ----------\n",
    "taus, ev = profit_curve(last[\"yva\"], last[\"pva\"], PROFIT_GOOD, LOSS_BAD, steps=201)\n",
    "best_tau = float(taus[int(ev.argmax())])\n",
    "pd.DataFrame({\"tau\":taus, \"expected_profit\":ev}).to_csv(f\"{ART}/profit_curve_last_fold_tree.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(taus, ev); plt.axvline(best_tau, ls=\"--\", label=f\"tau*={best_tau:.3f}\")\n",
    "plt.xlabel(\"Próg akceptacji (p < tau)\"); plt.ylabel(\"Oczekiwany zysk\")\n",
    "plt.title(\"Krzywa zysku — Decision Tree (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/profit_curve_last_fold_tree.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 7) Test OOT (ostatni miesiąc) + kalibracja isotonic ----------\n",
    "if \"issue_d\" in df.columns and df[\"issue_d\"].notna().any():\n",
    "    months = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    oot_mask = (months == uniq[-1])      # ostatni miesiąc jako OOT\n",
    "    train_mask = ~oot_mask\n",
    "else:\n",
    "    # fallback 80/20\n",
    "    idx = df.index.to_numpy()\n",
    "    cut = int(len(idx)*0.8)\n",
    "    train_mask = np.zeros(len(idx), dtype=bool); train_mask[:cut] = True\n",
    "    oot_mask = ~train_mask\n",
    "\n",
    "X_train, y_train = df.loc[train_mask, :], y[train_mask]\n",
    "X_oot,   y_oot   = df.loc[oot_mask,   :], y[oot_mask]\n",
    "\n",
    "tree_pipe.fit(X_train, y_train)\n",
    "calibrated = CalibratedClassifierCV(tree_pipe, cv=\"prefit\", method=\"isotonic\")\n",
    "calibrated.fit(last[\"Xva\"], last[\"yva\"])\n",
    "p_oot = calibrated.predict_proba(X_oot)[:,1]\n",
    "\n",
    "oot_metrics = {\n",
    "    \"AUC\": roc_auc_score(y_oot, p_oot),\n",
    "    \"PR_AUC\": average_precision_score(y_oot, p_oot),\n",
    "    \"KS\": ks_score(y_oot, p_oot),\n",
    "    \"Brier\": brier_score_loss(y_oot, p_oot),\n",
    "    \"LogLoss\": log_loss(y_oot, p_oot, labels=[0,1]),\n",
    "    \"ECE\": ece_score(y_oot, p_oot)\n",
    "}\n",
    "pd.Series(oot_metrics).to_csv(f\"{ART}/oot_metrics_tree.csv\", header=False)\n",
    "print(\"\\nMetryki OOT (drzewo):\\n\", pd.Series(oot_metrics).round(4))\n",
    "\n",
    "# ---------- 8) Tabela decylowa i KS (OOT) ----------\n",
    "dec_tab = decile_table(y_oot, p_oot, deciles=10)\n",
    "dec_tab.to_csv(f\"{ART}/decile_table_oot_tree.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(dec_tab[\"decile\"], dec_tab[\"ks\"], marker=\"o\")\n",
    "plt.xlabel(\"Decyl (1 = najwyższe ryzyko)\"); plt.ylabel(\"KS\")\n",
    "plt.title(\"KS po decylach — Decision Tree (OOT)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/ks_by_decile_oot_tree.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 9) Ważność cech + szkic drzewa ----------\n",
    "# dopasuj preprocessing na TRAIN, by uzyskać nazwy cech po transformacji\n",
    "pre_fitted = pre.fit(X_train)\n",
    "feat_names = pre_fitted.get_feature_names_out()\n",
    "\n",
    "# pobierz ważności z wewnątrz pipeline\n",
    "fitted_tree = tree_pipe.named_steps[\"clf\"]\n",
    "# uwaga: feature_importances_ odnosi się do wyjścia z preprocesora\n",
    "imp = getattr(fitted_tree, \"feature_importances_\", None)\n",
    "if imp is not None and len(imp) == len(feat_names):\n",
    "    imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": imp}).sort_values(\"importance\", ascending=False)\n",
    "    imp_df.to_csv(f\"{ART}/tree_feature_importance.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    top = imp_df.head(15)[::-1]\n",
    "    plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "    plt.title(\"Decision Tree — TOP 15 ważności cech\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{ART}/tree_feature_importance_top15.png\", dpi=160); plt.close()\n",
    "\n",
    "# szkic drzewa: aby wizualizacja była czytelna, zbuduj płytszą kopię (np. max_depth=4)\n",
    "small_tree = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", DecisionTreeClassifier(\n",
    "        criterion=\"gini\", max_depth=4, min_samples_leaf=200,\n",
    "        class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "])\n",
    "small_tree.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plot_tree(\n",
    "    small_tree.named_steps[\"clf\"],\n",
    "    filled=True, impurity=True, proportion=True,\n",
    "    max_depth=4, fontsize=8\n",
    ")\n",
    "plt.title(\"Decision Tree — szkic (max_depth=4)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/tree_sketch_depth4.png\", dpi=160); plt.close()\n",
    "\n",
    "print(f\"\\nArtefakty zapisano w: {os.path.abspath(ART)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
