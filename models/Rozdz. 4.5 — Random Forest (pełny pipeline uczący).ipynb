{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe96666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukasz.wrobel\\AppData\\Local\\Temp\\ipykernel_22648\\2943407291.py:53: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kolumn num: 11, kat: 3\n",
      "Najlepsze parametry: (200, None, 100, 'log2') AUC_mean= 0.7007\n",
      "Średnie metryki CV (RF):\n",
      " AUC        0.7007\n",
      "PR_AUC     0.3937\n",
      "KS         0.2921\n",
      "Brier      0.2120\n",
      "LogLoss    0.6107\n",
      "ECE        0.2198\n",
      "dtype: float64\n",
      "\n",
      "Metryki OOT (RF):\n",
      " AUC        0.7013\n",
      "PR_AUC     0.4592\n",
      "KS         0.2970\n",
      "Brier      0.1833\n",
      "LogLoss    0.5444\n",
      "ECE        0.0137\n",
      "dtype: float64\n",
      "\n",
      "Artefakty zapisano w: c:\\Users\\lukasz.wrobel\\Desktop\\PRACA MAGISTERSKA\\pliki\\artifacts\\artifacts_45_rf\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Rozdz. 4.5 — Random Forest (pełny pipeline uczący)\n",
    "# ===========================================================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, roc_curve\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ---------- ścieżki / artefakty ----------\n",
    "ART = \"artifacts_45_rf\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Ekonomia decyzji (dostosuj do realiów)\n",
    "PROFIT_GOOD = 1_000\n",
    "LOSS_BAD   = -5_000\n",
    "\n",
    "# Walidacja czasowa\n",
    "N_SPLITS_TIME = 3\n",
    "N_BINS_CALIB  = 10\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "# ---------- 1) dane ----------\n",
    "SNAP_PATH = Path(\"C:/Users/lukasz.wrobel/Desktop/PRACA MAGISTERSKA/pliki/artifacts/artifacts/engineered_snapshot.csv\")\n",
    "if not SNAP_PATH.exists():\n",
    "    SNAP_PATH = Path(\"engineered_snapshot.csv\")\n",
    "\n",
    "df = pd.read_csv(SNAP_PATH)\n",
    "if \"issue_d\" in df.columns:\n",
    "    df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "\n",
    "assert \"loan_status_bin\" in df.columns, \"Brak kolumny 'loan_status_bin' w snapshotcie.\"\n",
    "df[\"loan_status_bin\"] = pd.to_numeric(df[\"loan_status_bin\"], errors=\"coerce\")\n",
    "df = df.loc[df[\"loan_status_bin\"].isin([0,1])].copy()\n",
    "\n",
    "# y jako Series (zachowuje index -> później .loc)\n",
    "y = df[\"loan_status_bin\"].astype(\"int8\")\n",
    "\n",
    "# sanity — NaN/Inf w cechach\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# listy cech (bez gołych datetime)\n",
    "feature_cols = [c for c in df.columns if c != \"loan_status_bin\" and not pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n",
    "print(f\"#kolumn num: {len(num_cols)}, kat: {len(cat_cols)}\")\n",
    "\n",
    "# ---------- 2) helpery ----------\n",
    "def time_blocks(frame: pd.DataFrame, date_col=\"issue_d\", n_splits=N_SPLITS_TIME):\n",
    "    \"\"\"Zwraca listę (train_idx, valid_idx) rosnących bloków czasowych (po miesiącach).\"\"\"\n",
    "    if date_col not in frame.columns or frame[date_col].isna().all():\n",
    "        # fallback 80/20 bez czasu\n",
    "        idx = frame.index.to_numpy()\n",
    "        cut = int(len(idx)*0.8)\n",
    "        return [(idx[:cut], idx[cut:])]\n",
    "    months = frame[date_col].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    if len(uniq) < n_splits:\n",
    "        n_splits = max(2, len(uniq))\n",
    "    chunks = np.array_split(uniq, n_splits)\n",
    "    pairs = []\n",
    "    for i in range(1, len(chunks)):\n",
    "        tr_m = np.concatenate(chunks[:i])\n",
    "        va_m = chunks[i]\n",
    "        tr_idx = frame.index[months.isin(tr_m)]\n",
    "        va_idx = frame.index[months.isin(va_m)]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            pairs.append((tr_idx, va_idx))\n",
    "    return pairs\n",
    "\n",
    "def ks_score(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=20):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        m = (idx == b)\n",
    "        if m.sum()==0: \n",
    "            continue\n",
    "        ece += m.mean() * abs(y_prob[m].mean() - y_true[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def decile_table(y_true, y_prob, deciles=10):\n",
    "    d = pd.DataFrame({\"y\": y_true, \"p\": y_prob}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    d[\"decile\"] = pd.qcut(d.index, q=deciles, labels=False) + 1\n",
    "    tab = d.groupby(\"decile\").agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        bad=(\"y\",\"sum\"),\n",
    "        good=(\"y\", lambda s: (1-s).sum()),\n",
    "        prob_mean=(\"p\",\"mean\")\n",
    "    ).reset_index()\n",
    "    tab[\"bad_rate\"] = tab[\"bad\"]/tab[\"n\"]\n",
    "    total_bad, total_good = tab[\"bad\"].sum(), tab[\"good\"].sum()\n",
    "    tab[\"cum_bad\"]  = tab[\"bad\"].cumsum()/max(total_bad,1)\n",
    "    tab[\"cum_good\"] = tab[\"good\"].cumsum()/max(total_good,1)\n",
    "    tab[\"ks\"] = (tab[\"cum_bad\"] - tab[\"cum_good\"]).abs()\n",
    "    return tab\n",
    "\n",
    "def profit_curve(y_true, y_prob, profit_good=PROFIT_GOOD, loss_bad=LOSS_BAD, steps=201):\n",
    "    taus = np.linspace(0,1,steps)\n",
    "    ev = []\n",
    "    for t in taus:\n",
    "        acc = y_prob < t\n",
    "        tg = ((y_true==0) & acc).sum()\n",
    "        tb = ((y_true==1) & acc).sum()\n",
    "        ev.append(tg*profit_good + tb*loss_bad)\n",
    "    return taus, np.array(ev)\n",
    "\n",
    "# ---------- 3) preprocessing ----------\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True))  # RF nie wymaga skalowania\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols),\n",
    "     (\"cat\", cat_pipe, cat_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=100,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", rf_base)\n",
    "])\n",
    "\n",
    "# ---------- 4) walidacja czasowa + siatka hiperparametrów ----------\n",
    "param_grid = {\n",
    "    \"clf__n_estimators\": [100, 200],\n",
    "    \"clf__max_depth\": [None, 10, 14],\n",
    "    \"clf__min_samples_leaf\": [100, 200],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "folds = time_blocks(df, \"issue_d\", n_splits=N_SPLITS_TIME)\n",
    "grid_results, last = [], {}\n",
    "best_auc, best_params = -np.inf, None\n",
    "\n",
    "for n_est in param_grid[\"clf__n_estimators\"]:\n",
    "    for max_depth in param_grid[\"clf__max_depth\"]:\n",
    "        for min_leaf in param_grid[\"clf__min_samples_leaf\"]:\n",
    "            for mfeat in param_grid[\"clf__max_features\"]:\n",
    "                rf_pipe.set_params(\n",
    "                    clf__n_estimators=n_est,\n",
    "                    clf__max_depth=max_depth,\n",
    "                    clf__min_samples_leaf=min_leaf,\n",
    "                    clf__max_features=mfeat\n",
    "                )\n",
    "                fold_aucs = []\n",
    "                for tr_idx, va_idx in folds:\n",
    "                    Xtr, ytr = df.loc[tr_idx, :], y.loc[tr_idx]\n",
    "                    Xva, yva = df.loc[va_idx, :], y.loc[va_idx]\n",
    "                    rf_pipe.fit(Xtr, ytr)\n",
    "                    p = rf_pipe.predict_proba(Xva)[:,1]\n",
    "                    fold_aucs.append(roc_auc_score(yva, p))\n",
    "                    last = {\"Xtr\":Xtr, \"ytr\":ytr, \"Xva\":Xva, \"yva\":yva, \"pva\":p}\n",
    "                mean_auc = float(np.mean(fold_aucs))\n",
    "                grid_results.append({\n",
    "                    \"n_estimators\": n_est, \"max_depth\": max_depth,\n",
    "                    \"min_samples_leaf\": min_leaf, \"max_features\": mfeat,\n",
    "                    \"AUC_mean\": mean_auc\n",
    "                })\n",
    "                if mean_auc > best_auc:\n",
    "                    best_auc, best_params = mean_auc, (n_est, max_depth, min_leaf, mfeat)\n",
    "\n",
    "cv_grid = pd.DataFrame(grid_results).sort_values(\"AUC_mean\", ascending=False)\n",
    "cv_grid.to_csv(f\"{ART}/cv_grid_rf.csv\", index=False)\n",
    "print(\"Najlepsze parametry:\", best_params, \"AUC_mean=\", round(best_auc,4))\n",
    "\n",
    "# ustaw najlepsze i policz pełny zestaw metryk\n",
    "rf_pipe.set_params(\n",
    "    clf__n_estimators=600,\n",
    "    clf__max_depth=best_params[1],\n",
    "    clf__min_samples_leaf=best_params[2],\n",
    "    clf__max_features=best_params[3]\n",
    ")\n",
    "\n",
    "metrics = []\n",
    "for tr_idx, va_idx in folds:\n",
    "    Xtr, ytr = df.loc[tr_idx, :], y.loc[tr_idx]\n",
    "    Xva, yva = df.loc[va_idx, :], y.loc[va_idx]\n",
    "    rf_pipe.fit(Xtr, ytr)\n",
    "    p = rf_pipe.predict_proba(Xva)[:,1]\n",
    "    metrics.append({\n",
    "        \"AUC\": roc_auc_score(yva, p),\n",
    "        \"PR_AUC\": average_precision_score(yva, p),\n",
    "        \"KS\": ks_score(yva, p),\n",
    "        \"Brier\": brier_score_loss(yva, p),\n",
    "        \"LogLoss\": log_loss(yva, p, labels=[0,1]),\n",
    "        \"ECE\": ece_score(yva, p)\n",
    "    })\n",
    "    last = {\"Xtr\":Xtr, \"ytr\":ytr, \"Xva\":Xva, \"yva\":yva, \"pva\":p}\n",
    "\n",
    "cv_results = pd.DataFrame(metrics)\n",
    "cv_results.to_csv(f\"{ART}/cv_fold_metrics_rf.csv\", index=False)\n",
    "cv_mean = cv_results.mean()\n",
    "cv_mean.to_csv(f\"{ART}/cv_metrics_mean_rf.csv\", header=False)\n",
    "print(\"Średnie metryki CV (RF):\\n\", cv_mean.round(4))\n",
    "\n",
    "# ---------- 5) ROC i kalibracja (ostatni fold) ----------\n",
    "fpr, tpr, _ = roc_curve(last[\"yva\"], last[\"pva\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(last['yva'],last['pva']):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — Random Forest (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_last_fold_rf.png\", dpi=160); plt.close()\n",
    "\n",
    "frac_pos, mean_pred = calibration_curve(last[\"yva\"], last[\"pva\"], n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja — Random Forest (ostatni fold)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/calibration_last_fold_rf.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 6) Krzywa zysku + próg ----------\n",
    "taus, ev = profit_curve(last[\"yva\"], last[\"pva\"], PROFIT_GOOD, LOSS_BAD, steps=201)\n",
    "best_tau = float(taus[int(ev.argmax())])\n",
    "pd.DataFrame({\"tau\":taus, \"expected_profit\":ev}).to_csv(f\"{ART}/profit_curve_last_fold_rf.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(taus, ev); plt.axvline(best_tau, ls=\"--\", label=f\"tau*={best_tau:.3f}\")\n",
    "plt.xlabel(\"Próg akceptacji (p < tau)\"); plt.ylabel(\"Oczekiwany zysk\")\n",
    "plt.title(\"Krzywa zysku — Random Forest (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/profit_curve_last_fold_rf.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 7) Test OOT (ostatni miesiąc) + kalibracja isotonic ----------\n",
    "if \"issue_d\" in df.columns and df[\"issue_d\"].notna().any():\n",
    "    months = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    oot_mask = (months == uniq[-1])\n",
    "    train_mask = ~oot_mask\n",
    "else:\n",
    "    idx = df.index.to_numpy()\n",
    "    cut = int(len(idx)*0.8)\n",
    "    train_mask = np.zeros(len(idx), dtype=bool); train_mask[:cut] = True\n",
    "    oot_mask = ~train_mask\n",
    "\n",
    "X_train, y_train = df.loc[train_mask, :], y.loc[train_mask]\n",
    "X_oot,   y_oot   = df.loc[oot_mask,   :], y.loc[oot_mask]\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "calibrated = CalibratedClassifierCV(rf_pipe, cv=\"prefit\", method=\"isotonic\")\n",
    "calibrated.fit(last[\"Xva\"], last[\"yva\"])\n",
    "p_oot = calibrated.predict_proba(X_oot)[:,1]\n",
    "\n",
    "oot_metrics = {\n",
    "    \"AUC\": roc_auc_score(y_oot, p_oot),\n",
    "    \"PR_AUC\": average_precision_score(y_oot, p_oot),\n",
    "    \"KS\": ks_score(y_oot, p_oot),\n",
    "    \"Brier\": brier_score_loss(y_oot, p_oot),\n",
    "    \"LogLoss\": log_loss(y_oot, p_oot, labels=[0,1]),\n",
    "    \"ECE\": ece_score(y_oot, p_oot)\n",
    "}\n",
    "pd.Series(oot_metrics).to_csv(f\"{ART}/oot_metrics_rf.csv\", header=False)\n",
    "print(\"\\nMetryki OOT (RF):\\n\", pd.Series(oot_metrics).round(4))\n",
    "\n",
    "# ---------- 8) Tabela decylowa + KS (OOT) ----------\n",
    "dec_tab = decile_table(y_oot, p_oot, deciles=10)\n",
    "dec_tab.to_csv(f\"{ART}/decile_table_oot_rf.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(dec_tab[\"decile\"], dec_tab[\"ks\"], marker=\"o\")\n",
    "plt.xlabel(\"Decyl (1 = najwyższe ryzyko)\"); plt.ylabel(\"KS\")\n",
    "plt.title(\"KS po decylach — Random Forest (OOT)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/ks_by_decile_oot_rf.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 9) Ważność cech: Gini + Permutation ----------\n",
    "# nazwy cech po preprocesingu\n",
    "pre_fitted = rf_pipe.named_steps[\"pre\"].fit(X_train)\n",
    "feat_names = pre_fitted.get_feature_names_out()\n",
    "\n",
    "# Gini importance\n",
    "fitted_rf = rf_pipe.named_steps[\"clf\"]\n",
    "imp = getattr(fitted_rf, \"feature_importances_\", None)\n",
    "if imp is not None and len(imp) == len(feat_names):\n",
    "    imp_df = pd.DataFrame({\"feature\": feat_names, \"importance_gini\": imp}).sort_values(\"importance_gini\", ascending=False)\n",
    "    imp_df.to_csv(f\"{ART}/rf_feature_importance_gini.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    top = imp_df.head(15)[::-1]\n",
    "    plt.barh(top[\"feature\"], top[\"importance_gini\"])\n",
    "    plt.title(\"Random Forest — TOP 15 ważności (Gini)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{ART}/rf_feature_importance_gini_top15.png\", dpi=160); plt.close()\n",
    "\n",
    "# Permutation importance (na walidacji z ostatniego foldu)\n",
    "# załóżmy, że rf_pipe = Pipeline([(\"preprocess\", pre), (\"clf\", rf)])\n",
    "Xva_raw = last[\"Xva\"]      # DataFrame z oryginalnymi kolumnami\n",
    "yva     = last[\"yva\"]\n",
    "perm = permutation_importance(rf_pipe, Xva_raw, yva, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "# nazwy CECH PO transformacjach (OHE, passthrough, itd.)\n",
    "# znajdź w pipelinie krok ColumnTransformer (nazwa kroku może być inna niż \"preprocess\")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# --- wyciągamy kroki z pipeline ---\n",
    "# rf_pipe = Pipeline([(\"pre\", pre), (\"clf\", rf)])  # przykładowa struktura\n",
    "steps = dict(rf_pipe.named_steps)\n",
    "\n",
    "# znajdź ColumnTransformer (nazwa kroku u Ciebie może być inna)\n",
    "pre = None\n",
    "for name, step in steps.items():\n",
    "    if isinstance(step, ColumnTransformer):\n",
    "        pre = step\n",
    "        break\n",
    "if pre is None:\n",
    "    raise RuntimeError(\"Nie znaleziono ColumnTransformer w rf_pipe.\")\n",
    "\n",
    "# ostatni krok jako estymator (RandomForestClassifier)\n",
    "clf = steps.get(\"clf\", list(steps.values())[-1])\n",
    "\n",
    "# --- dane walidacyjne w oryginalnej postaci ---\n",
    "Xva_raw = last[\"Xva\"]\n",
    "yva     = last[\"yva\"]\n",
    "\n",
    "# --- transformacja walidacji dokładnie tym samym preprocessem ---\n",
    "Xva_enc = pre.transform(Xva_raw)\n",
    "\n",
    "# jeżeli wyjście jest rzadkie, a RF wymaga gęstej macierzy:\n",
    "if hasattr(Xva_enc, \"toarray\"):\n",
    "    try:\n",
    "        Xva_enc = Xva_enc.toarray()\n",
    "    except Exception:\n",
    "        pass  # jeśli już jest gęsta, nic nie rób\n",
    "\n",
    "# --- policz permutation importance na samym klasyfikatorze i zakodowanych cechach ---\n",
    "perm = permutation_importance(\n",
    "    clf, Xva_enc, yva,\n",
    "    n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- nazwy cech po transformacjach ---\n",
    "if hasattr(pre, \"get_feature_names_out\"):\n",
    "    feat_names = pre.get_feature_names_out()\n",
    "else:\n",
    "    feat_names = pre.get_feature_names()  # fallback dla starszego sklearn\n",
    "\n",
    "# bezpieczeństwo: dopasuj długości\n",
    "n = perm.importances_mean.shape[0]\n",
    "if len(feat_names) != n:\n",
    "    # spróbuj przyciąć / dopasować – lepiej niż wywalić się błędem\n",
    "    feat_names = list(feat_names)[:n]\n",
    "\n",
    "# --- zapis wyników ---\n",
    "perm_df = pd.DataFrame({\n",
    "    \"feature\": feat_names,\n",
    "    \"importance_perm_mean\": perm.importances_mean,\n",
    "    \"importance_perm_std\":  perm.importances_std\n",
    "}).sort_values(\"importance_perm_mean\", ascending=False)\n",
    "\n",
    "perm_df.to_csv(f\"{ART}/rf_feature_importance_permutation.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "top = perm_df.head(15)[::-1]\n",
    "plt.barh(top[\"feature\"], top[\"importance_perm_mean\"])\n",
    "plt.title(\"Random Forest — TOP 15 ważności (Permutation)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/rf_feature_importance_perm_top15.png\", dpi=160); plt.close()\n",
    "\n",
    "print(f\"\\nArtefakty zapisano w: {os.path.abspath(ART)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
