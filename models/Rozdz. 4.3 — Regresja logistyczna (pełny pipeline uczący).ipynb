{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f094b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-clean: start=2029952, po=1569804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukasz.wrobel\\AppData\\Local\\Temp\\ipykernel_5640\\935567528.py:77: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kolumn num: 12, kat: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie metryki CV (czasowej):\n",
      "AUC        0.6981\n",
      "PR_AUC     0.3832\n",
      "KS         0.2880\n",
      "Brier      0.2510\n",
      "LogLoss    0.6978\n",
      "ECE        0.2940\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metryki OOT (ostatni miesiąc):\n",
      "AUC        0.6853\n",
      "PR_AUC     0.4400\n",
      "KS         0.2675\n",
      "Brier      0.1863\n",
      "LogLoss    0.5532\n",
      "ECE        0.0139\n",
      "dtype: float64\n",
      "\n",
      "Najsilniejsze ujemne wpływy (TOP 15):\n",
      "                                          feature      beta        OR\n",
      "17         loan_to_income_qbin_(-0.000839, 0.075] -0.924569  0.396702\n",
      "36   installment_to_income_qbin_(0.0118, 1189.24] -0.653092  0.520434\n",
      "18             loan_to_income_qbin_(0.075, 0.109] -0.633439  0.530763\n",
      "35   installment_to_income_qbin_(0.00956, 0.0118] -0.440708  0.643580\n",
      "19              loan_to_income_qbin_(0.109, 0.14] -0.431890  0.649281\n",
      "1                                    int_rate_num -0.328287  0.720157\n",
      "34  installment_to_income_qbin_(0.00805, 0.00956] -0.268430  0.764579\n",
      "20              loan_to_income_qbin_(0.14, 0.169] -0.251028  0.778001\n",
      "33  installment_to_income_qbin_(0.00689, 0.00805] -0.137513  0.871523\n",
      "21               loan_to_income_qbin_(0.169, 0.2] -0.089739  0.914170\n",
      "46          revol_to_income_qbin_(0.444, 65324.0] -0.078055  0.924913\n",
      "4                                  annual_inc_log -0.054096  0.947342\n",
      "45             revol_to_income_qbin_(0.33, 0.444] -0.044318  0.956650\n",
      "44             revol_to_income_qbin_(0.264, 0.33] -0.040905  0.959920\n",
      "32  installment_to_income_qbin_(0.00593, 0.00689] -0.019355  0.980831\n",
      "\n",
      "Najsilniejsze dodatnie wpływy (TOP 15):\n",
      "                                              feature      beta        OR\n",
      "3                                      annual_inc_num  0.033186  1.033743\n",
      "22                   loan_to_income_qbin_(0.2, 0.234]  0.070641  1.073196\n",
      "37              revol_to_income_qbin_(-0.001, 0.0462]  0.071480  1.074097\n",
      "31      installment_to_income_qbin_(0.00505, 0.00593]  0.094904  1.099554\n",
      "0                                                  id  0.095573  1.100289\n",
      "11                                        issue_d_ord  0.108594  1.114710\n",
      "30      installment_to_income_qbin_(0.00422, 0.00505]  0.199419  1.220693\n",
      "23                 loan_to_income_qbin_(0.234, 0.275]  0.236580  1.266909\n",
      "29      installment_to_income_qbin_(0.00336, 0.00422]  0.295001  1.343128\n",
      "27  installment_to_income_qbin_(-0.00099468, 0.00237]  0.402592  1.495696\n",
      "28      installment_to_income_qbin_(0.00237, 0.00336]  0.405049  1.499377\n",
      "24                 loan_to_income_qbin_(0.275, 0.324]  0.421332  1.523990\n",
      "25                   loan_to_income_qbin_(0.324, 0.4]  0.641293  1.898934\n",
      "26                 loan_to_income_qbin_(0.4, 40000.0]  0.838686  2.313326\n",
      "2                                        int_rate_log  1.025697  2.789039\n",
      "\n",
      "Decyzje OOT przy tau*=0.375: accepted=9216, TG=7183, TB=2033, EV=-2,982,000\n",
      "\n",
      "Artefakty zapisano w: c:\\Users\\lukasz.wrobel\\Desktop\\PRACA MAGISTERSKA\\pliki\\artifacts\\artifacts_43\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Rozdz. 4.3 — Regresja logistyczna (pełny pipeline uczący)\n",
    "# ===========================================================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, roc_curve\n",
    ")\n",
    "\n",
    "# ---------- ustawienia ----------\n",
    "ART = \"artifacts_43\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "RANDOM_STATE   = 42\n",
    "N_SPLITS_TIME  = 6\n",
    "N_BINS_CALIB   = 10\n",
    "PROFIT_GOOD    = 1_000\n",
    "LOSS_BAD       = -5_000\n",
    "\n",
    "# ---------- 1) dane ----------\n",
    "SNAP_PATH = Path(\"C:/Users/lukasz.wrobel/Desktop/PRACA MAGISTERSKA/pliki/artifacts/artifacts/engineered_snapshot.csv\")\n",
    "if not SNAP_PATH.exists():\n",
    "    SNAP_PATH = Path(\"engineered_snapshot.csv\")\n",
    "\n",
    "df = pd.read_csv(SNAP_PATH)\n",
    "if \"issue_d\" in df.columns:\n",
    "    df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "\n",
    "assert \"loan_status_bin\" in df.columns, \"Brak kolumny loan_status_bin w snapshotcie.\"\n",
    "df[\"loan_status_bin\"] = pd.to_numeric(df[\"loan_status_bin\"], errors=\"coerce\")\n",
    "\n",
    "# fallback mapowanie, gdyby jednak nie było etykiet 0/1\n",
    "if df[\"loan_status_bin\"].isna().all() and \"loan_status\" in df.columns:\n",
    "    map_rules = {\n",
    "        \"Fully Paid\": 0,\n",
    "        \"Charged Off\": 1, \"Default\": 1,\n",
    "        \"Late (31-120 days)\": 1, \"Late (16-30 days)\": 1, \"In Grace Period\": 1,\n",
    "        \"Does not meet the credit policy. Status:Fully Paid\": 0,\n",
    "        \"Does not meet the credit policy. Status:Charged Off\": 1,\n",
    "        \"Current\": np.nan, \"Issued\": np.nan\n",
    "    }\n",
    "    df[\"loan_status_bin\"] = df[\"loan_status\"].map(map_rules)\n",
    "\n",
    "# filtrujemy tylko 0/1\n",
    "n0 = len(df)\n",
    "df = df.loc[df[\"loan_status_bin\"].isin([0, 1])].copy()\n",
    "print(f\"y-clean: start={n0}, po={len(df)}\")\n",
    "\n",
    "# zabezpieczenie cech\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# === KLUCZOWA ZMIANA ===\n",
    "# y jako Series (zachowuje index) – później używamy y.loc[...]\n",
    "y = df[\"loan_status_bin\"].astype(\"int8\")\n",
    "\n",
    "# pomocnicza kolumna czasowa (jeśli mamy datę)\n",
    "if \"issue_d\" in df.columns and pd.api.types.is_datetime64_any_dtype(df[\"issue_d\"]):\n",
    "    df[\"issue_d_ord\"] = (df[\"issue_d\"].dt.year * 12 + df[\"issue_d\"].dt.month).astype(\"int32\")\n",
    "\n",
    "# usuń całkowicie puste i stałe kolumny\n",
    "empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "if empty_cols:\n",
    "    df.drop(columns=empty_cols, inplace=True)\n",
    "zero_var = [c for c in df.columns if df[c].nunique(dropna=True) <= 1 and c != \"loan_status_bin\"]\n",
    "if zero_var:\n",
    "    df.drop(columns=zero_var, inplace=True)\n",
    "\n",
    "# listy cech (bez celu i bez daty wprost)\n",
    "target_col  = \"loan_status_bin\"\n",
    "feature_cols = [c for c in df.columns if c != target_col and not pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n",
    "print(f\"#kolumn num: {len(num_cols)}, kat: {len(cat_cols)}\")\n",
    "\n",
    "# ---------- 2) helpery ----------\n",
    "def time_blocks(frame, date_col=\"issue_d\", n_splits=N_SPLITS_TIME):\n",
    "    \"\"\"Zwraca listę (train_idx, valid_idx) z podziałem po miesiącach (rosnąco w czasie).\"\"\"\n",
    "    assert date_col in frame.columns, f\"Brak kolumny {date_col} do CV czasowego.\"\n",
    "    months = frame[date_col].dt.to_period(\"M\").astype(str)\n",
    "    uniq = months.sort_values().unique()\n",
    "    chunks = np.array_split(uniq, n_splits)\n",
    "    pairs = []\n",
    "    for i in range(1, len(chunks)):\n",
    "        train_months = np.concatenate(chunks[:i])\n",
    "        valid_months = chunks[i]\n",
    "        tr_idx = frame.index[months.isin(train_months)]\n",
    "        va_idx = frame.index[months.isin(valid_months)]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            pairs.append((tr_idx, va_idx))\n",
    "    return pairs\n",
    "\n",
    "def ks_score(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=20):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        m = idx==b\n",
    "        if m.sum()==0: \n",
    "            continue\n",
    "        ece += m.mean() * abs(y_prob[m].mean() - y_true[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def decile_table(y_true, y_prob, deciles=10):\n",
    "    df_ = pd.DataFrame({\"y\":y_true, \"p\":y_prob}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    df_[\"decile\"] = pd.qcut(df_.index, q=deciles, labels=False) + 1  # 1=top risk\n",
    "    tab = df_.groupby(\"decile\").agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        bad=(\"y\",\"sum\"),\n",
    "        good=(\"y\", lambda s: (1-s).sum()),\n",
    "        prob_mean=(\"p\",\"mean\")\n",
    "    ).reset_index()\n",
    "    tab[\"bad_rate\"] = tab[\"bad\"] / tab[\"n\"]\n",
    "    total_bad = tab[\"bad\"].sum()\n",
    "    total_good = tab[\"good\"].sum()\n",
    "    tab[\"cum_bad\"]  = tab[\"bad\"].cumsum() / max(total_bad, 1)\n",
    "    tab[\"cum_good\"] = tab[\"good\"].cumsum() / max(total_good,1)\n",
    "    tab[\"ks\"] = (tab[\"cum_bad\"] - tab[\"cum_good\"]).abs()\n",
    "    return tab\n",
    "\n",
    "def profit_curve(y_true, y_prob, profit_good=PROFIT_GOOD, loss_bad=LOSS_BAD, steps=101):\n",
    "    taus = np.linspace(0, 1, steps)\n",
    "    ev = []\n",
    "    for t in taus:\n",
    "        accept = y_prob < t\n",
    "        tg = ((y_true==0) & accept).sum()\n",
    "        tb = ((y_true==1) & accept).sum()\n",
    "        ev.append(tg*profit_good + tb*loss_bad)\n",
    "    return taus, np.array(ev)\n",
    "\n",
    "# ---------- 3) preprocesor + pipeline (bazowy) ----------\n",
    "# Uwaga: w pętli CV będziemy podawać konkretne listy kolumn (valid_num/valid_cat) – patrz niżej.\n",
    "def make_pipeline(num_list, cat_list):\n",
    "    # imputacja + skala dla num\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    # OHE dla kat (z imputacją najczęstszej kategorii)\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"__MISSING__\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        [(\"num\", num_pipe, num_list),\n",
    "         (\"cat\", cat_pipe, cat_list)],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    logit = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", LogisticRegression(solver=\"saga\",\n",
    "                                  max_iter=1000,\n",
    "                                  class_weight=\"balanced\",\n",
    "                                  penalty=\"l2\",\n",
    "                                  random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    return logit, pre\n",
    "\n",
    "# ---------- 4) walidacja czasowa ----------\n",
    "folds = time_blocks(df, \"issue_d\", n_splits=N_SPLITS_TIME)\n",
    "metrics, last = [], {}\n",
    "\n",
    "for tr_idx, va_idx in folds:\n",
    "    # tylko kolumny, które mają przynajmniej jedną nie-NaN w TRAIN:\n",
    "    valid_num = [c for c in num_cols if df.loc[tr_idx, c].notna().any()]\n",
    "    valid_cat = [c for c in cat_cols if df.loc[tr_idx, c].notna().any()]\n",
    "\n",
    "    logit, pre = make_pipeline(valid_num, valid_cat)\n",
    "\n",
    "    # === KLUCZOWA ZMIANA === używamy .loc do y (Series!)\n",
    "    Xtr, ytr = df.loc[tr_idx, valid_num+valid_cat], y.loc[tr_idx]\n",
    "    Xva, yva = df.loc[va_idx, valid_num+valid_cat], y.loc[va_idx]\n",
    "\n",
    "    logit.fit(Xtr, ytr)\n",
    "    p = logit.predict_proba(Xva)[:,1]\n",
    "\n",
    "    metrics.append({\n",
    "        \"AUC\":   roc_auc_score(yva, p),\n",
    "        \"PR_AUC\": average_precision_score(yva, p),\n",
    "        \"KS\":    ks_score(yva, p),\n",
    "        \"Brier\": brier_score_loss(yva, p),\n",
    "        \"LogLoss\": log_loss(yva, p, labels=[0,1]),\n",
    "        \"ECE\":   ece_score(yva, p)\n",
    "    })\n",
    "    last = {\"Xtr\":Xtr, \"ytr\":ytr, \"Xva\":Xva, \"yva\":yva, \"pva\":p, \"valid_num\":valid_num, \"valid_cat\":valid_cat}\n",
    "\n",
    "cv_results = pd.DataFrame(metrics)\n",
    "cv_mean = cv_results.mean()\n",
    "cv_results.to_csv(f\"{ART}/cv_fold_metrics.csv\", index=False)\n",
    "cv_mean.to_csv(f\"{ART}/cv_metrics_mean.csv\", header=False)\n",
    "print(\"Średnie metryki CV (czasowej):\")\n",
    "print(cv_mean.round(4))\n",
    "\n",
    "# ---------- 5) wykresy na ostatnim foldzie ----------\n",
    "fpr, tpr, _ = roc_curve(last[\"yva\"], last[\"pva\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(last['yva'],last['pva']):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — Logistic Regression (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_last_fold.png\", dpi=160); plt.close()\n",
    "\n",
    "frac_pos, mean_pred = calibration_curve(last[\"yva\"], last[\"pva\"], n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred, frac_pos, marker=\"o\", label=\"Observed vs predicted\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja — Logistic Regression (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/calibration_last_fold.png\", dpi=160); plt.close()\n",
    "\n",
    "taus, ev = profit_curve(last[\"yva\"], last[\"pva\"], PROFIT_GOOD, LOSS_BAD, steps=201)\n",
    "best_idx = int(ev.argmax())\n",
    "best_tau = float(taus[best_idx])\n",
    "best_ev  = float(ev.max())\n",
    "pd.DataFrame({\"tau\":taus, \"expected_profit\":ev}).to_csv(f\"{ART}/profit_curve_last_fold.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(taus, ev)\n",
    "plt.axvline(best_tau, ls=\"--\", label=f\"tau* = {best_tau:.3f}, EV={best_ev:,.0f}\")\n",
    "plt.xlabel(\"Próg akceptacji (p < tau)\"); plt.ylabel(\"Oczekiwany zysk (jednostki)\")\n",
    "plt.title(\"Krzywa zysku — wybór progu (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/profit_curve_last_fold.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 6) model finalny + kalibracja + ocena OOT ----------\n",
    "oot_months  = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n",
    "uniq_months = np.array(sorted(oot_months.unique()))\n",
    "oot_mask    = (oot_months == uniq_months[-1])\n",
    "train_mask  = ~oot_mask\n",
    "\n",
    "# używamy dokładnie tych samych list kolumn co w ostatnim foldzie (stabilność)\n",
    "valid_num = last[\"valid_num\"]; valid_cat = last[\"valid_cat\"]\n",
    "logit, pre = make_pipeline(valid_num, valid_cat)\n",
    "\n",
    "X_train, y_train = df.loc[train_mask, valid_num+valid_cat], y.loc[train_mask]\n",
    "X_oot,   y_oot   = df.loc[oot_mask,   valid_num+valid_cat], y.loc[oot_mask]\n",
    "\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# kalibracja isotonic na ostatnim foldzie\n",
    "calibrated = CalibratedClassifierCV(logit, cv=\"prefit\", method=\"isotonic\")\n",
    "calibrated.fit(last[\"Xva\"], last[\"yva\"])\n",
    "\n",
    "p_oot = calibrated.predict_proba(X_oot)[:,1]\n",
    "\n",
    "oot_metrics = {\n",
    "    \"AUC\":   roc_auc_score(y_oot, p_oot),\n",
    "    \"PR_AUC\": average_precision_score(y_oot, p_oot),\n",
    "    \"KS\":    ks_score(y_oot, p_oot),\n",
    "    \"Brier\": brier_score_loss(y_oot, p_oot),\n",
    "    \"LogLoss\": log_loss(y_oot, p_oot, labels=[0,1]),\n",
    "    \"ECE\":   ece_score(y_oot, p_oot)\n",
    "}\n",
    "pd.Series(oot_metrics).to_csv(f\"{ART}/oot_metrics.csv\", header=False)\n",
    "print(\"\\nMetryki OOT (ostatni miesiąc):\")\n",
    "print(pd.Series(oot_metrics).round(4))\n",
    "\n",
    "# ROC + kalibracja OOT\n",
    "fpr_o, tpr_o, _ = roc_curve(y_oot, p_oot)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr_o, tpr_o, label=f\"AUC={roc_auc_score(y_oot,p_oot):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — Logistic Regression (OOT)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_oot.png\", dpi=160); plt.close()\n",
    "\n",
    "frac_pos_o, mean_pred_o = calibration_curve(y_oot, p_oot, n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred_o, frac_pos_o, marker=\"o\", label=\"Observed vs predicted\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja — Logistic Regression (OOT)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/calibration_oot.png\", dpi=160); plt.close()\n",
    "\n",
    "# tabela decylowa + KS\n",
    "dec_tab = decile_table(y_oot, p_oot, deciles=10)\n",
    "dec_tab.to_csv(f\"{ART}/decile_table_oot.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(dec_tab[\"decile\"], dec_tab[\"ks\"], marker=\"o\")\n",
    "plt.xlabel(\"Decyl (1 = najwyższe ryzyko)\"); plt.ylabel(\"KS (kumul. różnica bad-good)\")\n",
    "plt.title(\"KS po decylach — OOT\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/ks_by_decile_oot.png\", dpi=160); plt.close()\n",
    "\n",
    "# współczynniki / OR (na całym train, po preprocesingu)\n",
    "pre_fitted = pre.fit(X_train)\n",
    "feat_names = pre_fitted.get_feature_names_out()\n",
    "clf = logit.named_steps[\"clf\"]\n",
    "coef = clf.coef_.ravel()\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"beta\": coef, \"OR\": np.exp(coef)}).sort_values(\"beta\")\n",
    "coef_df.to_csv(f\"{ART}/logit_coefficients_OR.csv\", index=False)\n",
    "print(\"\\nNajsilniejsze ujemne wpływy (TOP 15):\"); print(coef_df.head(15))\n",
    "print(\"\\nNajsilniejsze dodatnie wpływy (TOP 15):\"); print(coef_df.tail(15))\n",
    "\n",
    "# decyzje wg progu z walidacji\n",
    "accept_oot = (p_oot < best_tau)\n",
    "tg = int(((y_oot==0) & accept_oot).sum())\n",
    "tb = int(((y_oot==1) & accept_oot).sum())\n",
    "ev_oot = tg*PROFIT_GOOD + tb*LOSS_BAD\n",
    "pd.Series({\n",
    "    \"best_tau_from_valid\": best_tau,\n",
    "    \"accepted_cnt\": int(accept_oot.sum()),\n",
    "    \"true_good_accepted\": tg,\n",
    "    \"true_bad_accepted\": tb,\n",
    "    \"expected_profit_OOT\": ev_oot\n",
    "}).to_csv(f\"{ART}/decision_summary_oot.csv\", header=False)\n",
    "\n",
    "print(f\"\\nDecyzje OOT przy tau*={best_tau:.3f}: accepted={int(accept_oot.sum())}, TG={tg}, TB={tb}, EV={ev_oot:,.0f}\")\n",
    "print(f\"\\nArtefakty zapisano w: {os.path.abspath(ART)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
