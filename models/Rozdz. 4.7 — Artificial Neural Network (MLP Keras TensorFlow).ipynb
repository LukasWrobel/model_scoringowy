{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8b6f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukasz.wrobel\\AppData\\Local\\Temp\\ipykernel_18764\\4242246644.py:67: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kolumn num: 11, kat: 3\n",
      "WARNING:tensorflow:From c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Średnie metryki CV (ANN):\n",
      " AUC        0.7010\n",
      "PR_AUC     0.3874\n",
      "KS         0.2919\n",
      "Brier      0.2237\n",
      "LogLoss    0.6368\n",
      "ECE        0.2470\n",
      "dtype: float64\n",
      "Czas CV: 1505.2s\n",
      "\n",
      "Metryki OOT (ANN):\n",
      " AUC        0.7005\n",
      "PR_AUC     0.4601\n",
      "KS         0.2979\n",
      "Brier      0.1833\n",
      "LogLoss    0.5465\n",
      "ECE        0.0164\n",
      "dtype: float64\n",
      "\n",
      "Artefakty zapisano w: c:\\Users\\lukasz.wrobel\\Desktop\\PRACA MAGISTERSKA\\pliki\\artifacts\\artifacts_47_ann\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Rozdz. 4.7 — Artificial Neural Network (MLP, Keras/TensorFlow)\n",
    "# pełny pipeline: time-CV, metryki, kalibracja, OOT, artefakty\n",
    "# ===========================================================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "# --- TF/Keras ---\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers, callbacks\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Brak TensorFlow/Keras. Zainstaluj: pip install tensorflow\"\n",
    "    ) from e\n",
    "\n",
    "# --- sklearn ---\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, roc_curve\n",
    ")\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# ---------- ścieżki / artefakty ----------\n",
    "ART = \"artifacts_47_ann\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Ekonomia decyzji (dostosuj do realiów)\n",
    "PROFIT_GOOD = 1_000\n",
    "LOSS_BAD   = -5_000\n",
    "\n",
    "# Walidacja czasowa\n",
    "N_SPLITS_TIME = 6\n",
    "N_BINS_CALIB  = 10\n",
    "RANDOM_STATE  = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "# ---------- 1) dane ----------\n",
    "SNAP_PATH = Path(\"C:/Users/lukasz.wrobel/Desktop/PRACA MAGISTERSKA/pliki/artifacts/artifacts/engineered_snapshot.csv\")\n",
    "if not SNAP_PATH.exists():\n",
    "    SNAP_PATH = Path(\"engineered_snapshot.csv\")\n",
    "\n",
    "df = pd.read_csv(SNAP_PATH)\n",
    "if \"issue_d\" in df.columns:\n",
    "    df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "\n",
    "assert \"loan_status_bin\" in df.columns, \"Brak kolumny 'loan_status_bin' w snapshotcie.\"\n",
    "df[\"loan_status_bin\"] = pd.to_numeric(df[\"loan_status_bin\"], errors=\"coerce\")\n",
    "df = df.loc[df[\"loan_status_bin\"].isin([0,1])].copy()\n",
    "\n",
    "# y jako Series (zachowuje index -> później .loc)\n",
    "y = df[\"loan_status_bin\"].astype(\"int8\")\n",
    "\n",
    "# sanity — NaN/Inf w cechach\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# listy cech (bez gołych datetime)\n",
    "feature_cols = [c for c in df.columns if c != \"loan_status_bin\" and not pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n",
    "print(f\"#kolumn num: {len(num_cols)}, kat: {len(cat_cols)}\")\n",
    "\n",
    "# ---------- 2) helpery ----------\n",
    "def time_blocks(frame: pd.DataFrame, date_col=\"issue_d\", n_splits=N_SPLITS_TIME):\n",
    "    \"\"\"Zwraca listę (train_idx, valid_idx) rosnących bloków czasowych (po miesiącach).\"\"\"\n",
    "    if date_col not in frame.columns or frame[date_col].isna().all():\n",
    "        idx = frame.index.to_numpy()\n",
    "        cut = int(len(idx)*0.8)\n",
    "        return [(idx[:cut], idx[cut:])]\n",
    "    months = frame[date_col].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    if len(uniq) < n_splits:\n",
    "        n_splits = max(2, len(uniq))\n",
    "    chunks = np.array_split(uniq, n_splits)\n",
    "    pairs = []\n",
    "    for i in range(1, len(chunks)):\n",
    "        tr_m = np.concatenate(chunks[:i])\n",
    "        va_m = chunks[i]\n",
    "        tr_idx = frame.index[months.isin(tr_m)]\n",
    "        va_idx = frame.index[months.isin(va_m)]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            pairs.append((tr_idx, va_idx))\n",
    "    return pairs\n",
    "\n",
    "def ks_score(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=20):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        m = (idx == b)\n",
    "        if m.sum()==0: \n",
    "            continue\n",
    "        ece += m.mean() * abs(y_prob[m].mean() - y_true[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def decile_table(y_true, y_prob, deciles=10):\n",
    "    d = pd.DataFrame({\"y\": y_true, \"p\": y_prob}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    d[\"decile\"] = pd.qcut(d.index, q=deciles, labels=False) + 1\n",
    "    tab = d.groupby(\"decile\").agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        bad=(\"y\",\"sum\"),\n",
    "        good=(\"y\", lambda s: (1-s).sum()),\n",
    "        prob_mean=(\"p\",\"mean\")\n",
    "    ).reset_index()\n",
    "    tab[\"bad_rate\"] = tab[\"bad\"]/tab[\"n\"]\n",
    "    total_bad, total_good = tab[\"bad\"].sum(), tab[\"good\"].sum()\n",
    "    tab[\"cum_bad\"]  = tab[\"bad\"].cumsum()/max(total_bad,1)\n",
    "    tab[\"cum_good\"] = tab[\"good\"].cumsum()/max(total_good,1)\n",
    "    tab[\"ks\"] = (tab[\"cum_bad\"] - tab[\"cum_good\"]).abs()\n",
    "    return tab\n",
    "\n",
    "def profit_curve(y_true, y_prob, profit_good=PROFIT_GOOD, loss_bad=LOSS_BAD, steps=201):\n",
    "    taus = np.linspace(0,1,steps)\n",
    "    ev = []\n",
    "    for t in taus:\n",
    "        acc = y_prob < t\n",
    "        tg = ((y_true==0) & acc).sum()\n",
    "        tb = ((y_true==1) & acc).sum()\n",
    "        ev.append(tg*profit_good + tb*loss_bad)\n",
    "    return taus, np.array(ev)\n",
    "\n",
    "# ---------- 3) preprocessing ----------\n",
    "# ANN wymaga skalowania cech numerycznych -> StandardScaler\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols),\n",
    "     (\"cat\", cat_pipe, cat_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# ---------- 4) Architektura MLP ----------\n",
    "def build_mlp(input_dim: int, l2=1e-4, p1=0.30, p2=0.20, lr=1e-3):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,), dtype=\"float32\"),\n",
    "        layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dropout(p1),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dropout(p2),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(l2)),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "    return model\n",
    "\n",
    "def train_mlp(Xtr, ytr, Xva, yva, class_weight=None, lr=1e-3, epochs=100, batch=2048, patience=8):\n",
    "    model = build_mlp(Xtr.shape[1], lr=lr)\n",
    "    cb = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True, verbose=0),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=max(3, patience//2), min_lr=1e-5, verbose=0)\n",
    "    ]\n",
    "    hist = model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_data=(Xva, yva),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch,\n",
    "        class_weight=class_weight,\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "    return model, hist\n",
    "\n",
    "def plot_learning_curves(history, out_png):\n",
    "    h = history.history\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h[\"loss\"], label=\"train loss\")\n",
    "    plt.plot(h[\"val_loss\"], label=\"val loss\")\n",
    "    if \"auc\" in h and \"val_auc\" in h:\n",
    "        plt.plot(h[\"auc\"], label=\"train AUC\")\n",
    "        plt.plot(h[\"val_auc\"], label=\"val AUC\")\n",
    "    plt.xlabel(\"Epoka\"); plt.title(\"Krzywe uczenia — MLP\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# ---------- 5) Walidacja czasowa ----------\n",
    "folds = time_blocks(df, \"issue_d\", n_splits=N_SPLITS_TIME)\n",
    "metrics, last = [], {}\n",
    "t0 = time()\n",
    "\n",
    "for tr_idx, va_idx in folds:\n",
    "    # fit preprocessing na TRAIN i transformuj do macierzy wejściowej MLP\n",
    "    pre_fitted = pre.fit(df.loc[tr_idx, :])\n",
    "    Xtr = pre_fitted.transform(df.loc[tr_idx, :]).astype(\"float32\")\n",
    "    Xva = pre_fitted.transform(df.loc[va_idx, :]).astype(\"float32\")\n",
    "    feat_names = pre_fitted.get_feature_names_out()\n",
    "\n",
    "    ytr = y.loc[tr_idx].to_numpy()\n",
    "    yva = y.loc[va_idx].to_numpy()\n",
    "\n",
    "    # class weights (imbalance): wagi ∝ #neg / #pos\n",
    "    pos, neg = int((ytr==1).sum()), int((ytr==0).sum())\n",
    "    cw = {0: 1.0, 1: (neg / max(pos,1))}\n",
    "\n",
    "    model, hist = train_mlp(Xtr, ytr, Xva, yva, class_weight=cw, lr=1e-3, epochs=60, batch=2048, patience=8)\n",
    "    p = model.predict(Xva, batch_size=4096, verbose=0).ravel()\n",
    "\n",
    "    # metryki\n",
    "    metrics.append({\n",
    "        \"AUC\": roc_auc_score(yva, p),\n",
    "        \"PR_AUC\": average_precision_score(yva, p),\n",
    "        \"KS\": ks_score(yva, p),\n",
    "        \"Brier\": brier_score_loss(yva, p),\n",
    "        \"LogLoss\": log_loss(yva, p, labels=[0,1]),\n",
    "        \"ECE\": ece_score(yva, p)\n",
    "    })\n",
    "\n",
    "    last = {\"pre\":pre_fitted, \"Xva\":Xva, \"yva\":yva, \"pva\":p, \"feat_names\":feat_names,\n",
    "            \"hist\":hist, \"model\":model}\n",
    "\n",
    "cv_results = pd.DataFrame(metrics)\n",
    "cv_results.to_csv(f\"{ART}/cv_fold_metrics_ann.csv\", index=False)\n",
    "cv_mean = cv_results.mean()\n",
    "cv_mean.to_csv(f\"{ART}/cv_metrics_mean_ann.csv\", header=False)\n",
    "print(\"Średnie metryki CV (ANN):\\n\", cv_mean.round(4))\n",
    "print(f\"Czas CV: {time()-t0:.1f}s\")\n",
    "\n",
    "# ---------- 6) ROC i kalibracja (ostatni fold, przed i po) ----------\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(last[\"yva\"], last[\"pva\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(last['yva'],last['pva']):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — ANN (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_last_fold_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# krzywe uczenia\n",
    "plot_learning_curves(last[\"hist\"], f\"{ART}/learning_curves_last_fold_ann.png\")\n",
    "\n",
    "# kalibracja — przed\n",
    "frac_pos, mean_pred = calibration_curve(last[\"yva\"], last[\"pva\"], n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred, frac_pos, marker=\"o\", label=\"Observed\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja (przed) — ANN (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/calibration_before_last_fold_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# dopasuj isotonic na (p, y) z walidacji\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(last[\"pva\"], last[\"yva\"])\n",
    "pva_cal = iso.transform(last[\"pva\"])\n",
    "\n",
    "# kalibracja — po\n",
    "frac_pos_c, mean_pred_c = calibration_curve(last[\"yva\"], pva_cal, n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred_c, frac_pos_c, marker=\"o\", label=\"Observed (calibrated)\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja (po) — ANN (ostatni fold)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/calibration_after_last_fold_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 7) Krzywa zysku + próg (ostatni fold, po kalibracji) ----------\n",
    "taus, ev = profit_curve(last[\"yva\"], pva_cal, PROFIT_GOOD, LOSS_BAD, steps=201)\n",
    "best_tau = float(taus[int(ev.argmax())])\n",
    "pd.DataFrame({\"tau\":taus, \"expected_profit\":ev}).to_csv(f\"{ART}/profit_curve_last_fold_ann.csv\", index=False)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(taus, ev); plt.axvline(best_tau, ls=\"--\", label=f\"tau*={best_tau:.3f}\")\n",
    "plt.xlabel(\"Próg akceptacji (p < tau)\"); plt.ylabel(\"Oczekiwany zysk\")\n",
    "plt.title(\"Krzywa zysku — ANN (ostatni fold, po kalibracji)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/profit_curve_last_fold_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 8) Test OOT (ostatni miesiąc) ----------\n",
    "if \"issue_d\" in df.columns and df[\"issue_d\"].notna().any():\n",
    "    months = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    oot_mask = (months == uniq[-1])\n",
    "    train_mask = ~oot_mask\n",
    "else:\n",
    "    idx = df.index.to_numpy()\n",
    "    cut = int(len(idx)*0.8)\n",
    "    train_mask = np.zeros(len(idx), dtype=bool); train_mask[:cut] = True\n",
    "    oot_mask = ~train_mask\n",
    "\n",
    "# fit pre na TRAIN i transformacje\n",
    "pre_train = pre.fit(df.loc[train_mask, :])\n",
    "X_train = pre_train.transform(df.loc[train_mask, :]).astype(\"float32\")\n",
    "X_oot   = pre_train.transform(df.loc[oot_mask,   :]).astype(\"float32\")\n",
    "y_train = y.loc[train_mask].to_numpy()\n",
    "y_oot   = y.loc[oot_mask].to_numpy()\n",
    "\n",
    "# class weights na TRAIN\n",
    "pos_tr, neg_tr = int((y_train==1).sum()), int((y_train==0).sum())\n",
    "cw_train = {0: 1.0, 1: (neg_tr / max(pos_tr,1))}\n",
    "\n",
    "# ucz model finalny (early stopping na wew. walidacji 10%)\n",
    "model_final, hist_final = train_mlp(\n",
    "    X_train, y_train,\n",
    "    X_train[int(0.9*len(X_train)):], y_train[int(0.9*len(y_train)):],  # prosty val-split na końcówce\n",
    "    class_weight=cw_train, lr=8e-4, epochs=80, batch=4096, patience=8\n",
    ")\n",
    "\n",
    "# kalibracja isotonic — użyj tej z ostatniego folda (spójnie z §4.3–4.6)\n",
    "# (iso już dopasowany na (pva, yva))\n",
    "p_oot_raw = model_final.predict(X_oot, batch_size=4096, verbose=0).ravel()\n",
    "p_oot = iso.transform(p_oot_raw)\n",
    "\n",
    "# metryki OOT\n",
    "oot_metrics = {\n",
    "    \"AUC\": roc_auc_score(y_oot, p_oot),\n",
    "    \"PR_AUC\": average_precision_score(y_oot, p_oot),\n",
    "    \"KS\": ks_score(y_oot, p_oot),\n",
    "    \"Brier\": brier_score_loss(y_oot, p_oot),\n",
    "    \"LogLoss\": log_loss(y_oot, p_oot, labels=[0,1]),\n",
    "    \"ECE\": ece_score(y_oot, p_oot)\n",
    "}\n",
    "pd.Series(oot_metrics).to_csv(f\"{ART}/oot_metrics_ann.csv\", header=False)\n",
    "print(\"\\nMetryki OOT (ANN):\\n\", pd.Series(oot_metrics).round(4))\n",
    "\n",
    "# ROC/kalibracja OOT\n",
    "fpr_o, tpr_o, _ = roc_curve(y_oot, p_oot)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr_o, tpr_o, label=f\"AUC={roc_auc_score(y_oot,p_oot):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — ANN (OOT)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_oot_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "frac_pos_o, mean_pred_o = calibration_curve(y_oot, p_oot, n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_pred_o, frac_pos_o, marker=\"o\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "plt.title(\"Kalibracja — ANN (OOT)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/calibration_oot_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# tabela decylowa i KS po decylach (OOT)\n",
    "dec_tab = decile_table(y_oot, p_oot, deciles=10)\n",
    "dec_tab.to_csv(f\"{ART}/decile_table_oot_ann.csv\", index=False)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(dec_tab[\"decile\"], dec_tab[\"ks\"], marker=\"o\")\n",
    "plt.xlabel(\"Decyl (1 = najwyższe ryzyko)\"); plt.ylabel(\"KS\")\n",
    "plt.title(\"KS po decylach — ANN (OOT)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/ks_by_decile_oot_ann.png\", dpi=160); plt.close()\n",
    "\n",
    "# ---------- 9) Permutation Importance (ostatni fold, po preprocesingu) ----------\n",
    "# Prosta implementacja PI (spadek AUC po losowej permutacji kolumny)\n",
    "def permutation_importance_ann(model, X_val, y_val, feat_names, n_repeats=3, batch=4096, seed=RANDOM_STATE):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    base = roc_auc_score(y_val, model.predict(X_val, batch_size=batch, verbose=0).ravel())\n",
    "    imps = []\n",
    "    Xc = X_val.copy()\n",
    "    for j in range(Xc.shape[1]):\n",
    "        drops = []\n",
    "        for _ in range(n_repeats):\n",
    "            col = Xc[:, j].copy()\n",
    "            rng.shuffle(col)\n",
    "            Xc[:, j] = col\n",
    "            p = model.predict(Xc, batch_size=batch, verbose=0).ravel()\n",
    "            drops.append(base - roc_auc_score(y_val, p))\n",
    "            Xc[:, j] = X_val[:, j]  # restore\n",
    "        imps.append(np.mean(drops))\n",
    "    imp_df = pd.DataFrame({\"feature\": feat_names, \"importance_perm\": imps}).sort_values(\"importance_perm\", ascending=False)\n",
    "    return imp_df\n",
    "\n",
    "# licz PI na ostatnim foldzie i zapisz TOP-15\n",
    "imp_df = permutation_importance_ann(last[\"model\"], last[\"Xva\"], last[\"yva\"], last[\"feat_names\"], n_repeats=3)\n",
    "imp_df.to_csv(f\"{ART}/ann_feature_importance_permutation.csv\", index=False)\n",
    "plt.figure(figsize=(8,6))\n",
    "top = imp_df.head(15)[::-1]\n",
    "plt.barh(top[\"feature\"], top[\"importance_perm\"])\n",
    "plt.title(\"ANN — TOP 15 ważności (Permutation)\")\n",
    "plt.tight_layout(); plt.savefig(f\"{ART}/ann_feature_importance_perm_top15.png\", dpi=160); plt.close()\n",
    "\n",
    "print(f\"\\nArtefakty zapisano w: {os.path.abspath(ART)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
