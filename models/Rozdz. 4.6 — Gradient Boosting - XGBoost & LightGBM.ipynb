{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01561736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukasz.wrobel\\AppData\\Local\\Temp\\ipykernel_20164\\109027187.py:65: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kolumn num: 11, kat: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 35432, number of negative: 164836\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2493\n",
      "[LightGBM] [Info] Number of data points in the train set: 200268, number of used features: 41\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176923 -> initscore=-1.537336\n",
      "[LightGBM] [Info] Start training from score -1.537336\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 92001, number of negative: 398578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 490579, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187536 -> initscore=-1.466104\n",
      "[LightGBM] [Info] Start training from score -1.466104\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 165476, number of negative: 690026\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2741\n",
      "[LightGBM] [Info] Number of data points in the train set: 855502, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.193426 -> initscore=-1.427903\n",
      "[LightGBM] [Info] Start training from score -1.427903\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 223335, number of negative: 934018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2775\n",
      "[LightGBM] [Info] Number of data points in the train set: 1157353, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.192971 -> initscore=-1.430823\n",
      "[LightGBM] [Info] Start training from score -1.430823\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 283145, number of negative: 1116387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 1399532, number of used features: 46\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202314 -> initscore=-1.371894\n",
      "[LightGBM] [Info] Start training from score -1.371894\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 35432, number of negative: 164836\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2493\n",
      "[LightGBM] [Info] Number of data points in the train set: 200268, number of used features: 41\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176923 -> initscore=-1.537336\n",
      "[LightGBM] [Info] Start training from score -1.537336\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 92001, number of negative: 398578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 490579, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187536 -> initscore=-1.466104\n",
      "[LightGBM] [Info] Start training from score -1.466104\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 165476, number of negative: 690026\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2741\n",
      "[LightGBM] [Info] Number of data points in the train set: 855502, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.193426 -> initscore=-1.427903\n",
      "[LightGBM] [Info] Start training from score -1.427903\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 223335, number of negative: 934018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2775\n",
      "[LightGBM] [Info] Number of data points in the train set: 1157353, number of used features: 43\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.192971 -> initscore=-1.430823\n",
      "[LightGBM] [Info] Start training from score -1.430823\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 283145, number of negative: 1116387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 1399532, number of used features: 46\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202314 -> initscore=-1.371894\n",
      "[LightGBM] [Info] Start training from score -1.371894\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "XGB best: {'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 5, 'gamma': 0.0, 'reg_lambda': 1.0} AUC_mean= 0.7031\n",
      "LGB best: {'n_estimators': 1200, 'learning_rate': 0.04, 'num_leaves': 63, 'max_depth': -1, 'min_data_in_leaf': 200, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1} AUC_mean= 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukasz.wrobel\\Python\\Python3117\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie metryki CV XGB:\n",
      " AUC        0.7031\n",
      "PR_AUC     0.3910\n",
      "KS         0.2962\n",
      "Brier      0.2198\n",
      "LogLoss    0.6274\n",
      "ECE        0.2404\n",
      "dtype: float64\n",
      "Średnie metryki CV LGB:\n",
      " AUC        0.6980\n",
      "PR_AUC     0.3857\n",
      "KS         0.2880\n",
      "Brier      0.2150\n",
      "LogLoss    0.6161\n",
      "ECE        0.2236\n",
      "dtype: float64\n",
      "\n",
      "Metryki OOT (xgb):\n",
      " AUC        0.7052\n",
      "PR_AUC     0.4642\n",
      "KS         0.2996\n",
      "Brier      0.1823\n",
      "LogLoss    0.5418\n",
      "ECE        0.0156\n",
      "dtype: float64\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 327164, number of negative: 1230163\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1557327, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210080 -> initscore=-1.324440\n",
      "[LightGBM] [Info] Start training from score -1.324440\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "Metryki OOT (lgb):\n",
      " AUC        0.7057\n",
      "PR_AUC     0.4659\n",
      "KS         0.2996\n",
      "Brier      0.1821\n",
      "LogLoss    0.5418\n",
      "ECE        0.0148\n",
      "dtype: float64\n",
      "\n",
      "Artefakty zapisano w: c:\\Users\\lukasz.wrobel\\Desktop\\PRACA MAGISTERSKA\\pliki\\artifacts\\artifacts_46_gbm\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Rozdz. 4.6 — Gradient Boosting: XGBoost & LightGBM\n",
    "# pełny pipeline: walidacja czasowa, kalibracja, OOT, artefakty\n",
    "# ===========================================================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, roc_curve\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# ---------- ścieżki / artefakty ----------\n",
    "ART = \"artifacts_46_gbm\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Ekonomia decyzji (dostosuj do realiów)\n",
    "PROFIT_GOOD = 1_000\n",
    "LOSS_BAD   = -5_000\n",
    "\n",
    "# Walidacja czasowa\n",
    "N_SPLITS_TIME = 6\n",
    "N_BINS_CALIB  = 10\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "# ---------- 0) zależności zewnętrzne: xgboost i lightgbm ----------\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Brak pakietu 'xgboost'. Zainstaluj: pip install xgboost\") from e\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Brak pakietu 'lightgbm'. Zainstaluj: pip install lightgbm\") from e\n",
    "\n",
    "# ---------- 1) dane ----------\n",
    "SNAP_PATH = Path(\"C:/Users/lukasz.wrobel/Desktop/PRACA MAGISTERSKA/pliki/artifacts/artifacts/engineered_snapshot.csv\")\n",
    "if not SNAP_PATH.exists():\n",
    "    SNAP_PATH = Path(\"engineered_snapshot.csv\")\n",
    "\n",
    "df = pd.read_csv(SNAP_PATH)\n",
    "if \"issue_d\" in df.columns:\n",
    "    df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "\n",
    "assert \"loan_status_bin\" in df.columns, \"Brak kolumny 'loan_status_bin' w snapshotcie.\"\n",
    "df[\"loan_status_bin\"] = pd.to_numeric(df[\"loan_status_bin\"], errors=\"coerce\")\n",
    "df = df.loc[df[\"loan_status_bin\"].isin([0,1])].copy()\n",
    "\n",
    "# y jako Series (zachowuje index -> później .loc)\n",
    "y = df[\"loan_status_bin\"].astype(\"int8\")\n",
    "\n",
    "# sanity — NaN/Inf w cechach\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# listy cech (bez gołych datetime)\n",
    "feature_cols = [c for c in df.columns if c != \"loan_status_bin\" and not pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "cat_cols = [c for c in feature_cols if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c])]\n",
    "print(f\"#kolumn num: {len(num_cols)}, kat: {len(cat_cols)}\")\n",
    "\n",
    "# ---------- 2) helpery ----------\n",
    "def time_blocks(frame: pd.DataFrame, date_col=\"issue_d\", n_splits=N_SPLITS_TIME):\n",
    "    \"\"\"Zwraca listę (train_idx, valid_idx) rosnących bloków czasowych (po miesiącach).\"\"\"\n",
    "    if date_col not in frame.columns or frame[date_col].isna().all():\n",
    "        # fallback 80/20 bez czasu\n",
    "        idx = frame.index.to_numpy()\n",
    "        cut = int(len(idx)*0.8)\n",
    "        return [(idx[:cut], idx[cut:])]\n",
    "    months = frame[date_col].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    if len(uniq) < n_splits:\n",
    "        n_splits = max(2, len(uniq))\n",
    "    chunks = np.array_split(uniq, n_splits)\n",
    "    pairs = []\n",
    "    for i in range(1, len(chunks)):\n",
    "        tr_m = np.concatenate(chunks[:i])\n",
    "        va_m = chunks[i]\n",
    "        tr_idx = frame.index[months.isin(tr_m)]\n",
    "        va_idx = frame.index[months.isin(va_m)]\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            pairs.append((tr_idx, va_idx))\n",
    "    return pairs\n",
    "\n",
    "def ks_score(y_true, y_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=20):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        m = (idx == b)\n",
    "        if m.sum()==0: \n",
    "            continue\n",
    "        ece += m.mean() * abs(y_prob[m].mean() - y_true[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def decile_table(y_true, y_prob, deciles=10):\n",
    "    d = pd.DataFrame({\"y\": y_true, \"p\": y_prob}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    d[\"decile\"] = pd.qcut(d.index, q=deciles, labels=False) + 1\n",
    "    tab = d.groupby(\"decile\").agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        bad=(\"y\",\"sum\"),\n",
    "        good=(\"y\", lambda s: (1-s).sum()),\n",
    "        prob_mean=(\"p\",\"mean\")\n",
    "    ).reset_index()\n",
    "    tab[\"bad_rate\"] = tab[\"bad\"]/tab[\"n\"]\n",
    "    total_bad, total_good = tab[\"bad\"].sum(), tab[\"good\"].sum()\n",
    "    tab[\"cum_bad\"]  = tab[\"bad\"].cumsum()/max(total_bad,1)\n",
    "    tab[\"cum_good\"] = tab[\"good\"].cumsum()/max(total_good,1)\n",
    "    tab[\"ks\"] = (tab[\"cum_bad\"] - tab[\"cum_good\"]).abs()\n",
    "    return tab\n",
    "\n",
    "def profit_curve(y_true, y_prob, profit_good=PROFIT_GOOD, loss_bad=LOSS_BAD, steps=201):\n",
    "    taus = np.linspace(0,1,steps)\n",
    "    ev = []\n",
    "    for t in taus:\n",
    "        acc = y_prob < t\n",
    "        tg = ((y_true==0) & acc).sum()\n",
    "        tb = ((y_true==1) & acc).sum()\n",
    "        ev.append(tg*profit_good + tb*loss_bad)\n",
    "    return taus, np.array(ev)\n",
    "\n",
    "# ---------- 3) preprocessing (wspólny) ----------\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True))  # GBM nie wymaga skalowania\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols),\n",
    "     (\"cat\", cat_pipe, cat_cols)],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Uwaga: dla early_stopping musimy podać eval_set już po transformacji,\n",
    "# więc w pętli fitujemy pre na TRAIN, transformujemy X i dopiero trenujemy booster.\n",
    "\n",
    "# ---------- 4) XGBoost i LightGBM — siatki parametrów ----------\n",
    "xgb_grid = [\n",
    "    dict(\n",
    "        n_estimators=800, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        min_child_weight=5, gamma=0.0, reg_lambda=1.0\n",
    "    ),\n",
    "    dict(\n",
    "        n_estimators=600, max_depth=5, learning_rate=0.07,\n",
    "        subsample=0.8, colsample_bytree=0.7,\n",
    "        min_child_weight=3, gamma=0.0, reg_lambda=1.0\n",
    "    )\n",
    "]\n",
    "\n",
    "lgb_grid = [\n",
    "    dict(\n",
    "        n_estimators=1200, learning_rate=0.04,\n",
    "        num_leaves=63, max_depth=-1,\n",
    "        min_data_in_leaf=200, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1\n",
    "    ),\n",
    "    dict(\n",
    "        n_estimators=900, learning_rate=0.06,\n",
    "        num_leaves=63, max_depth=-1,\n",
    "        min_data_in_leaf=150, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ---------- 5) pętla walidacji czasowej ----------\n",
    "folds = time_blocks(df, \"issue_d\", n_splits=N_SPLITS_TIME)\n",
    "\n",
    "def run_model(model_name, param_list):\n",
    "    results_grid = []\n",
    "    best_auc, best_cfg, last = -np.inf, None, {}\n",
    "\n",
    "    for params in param_list:\n",
    "        fold_aucs = []\n",
    "        for tr_idx, va_idx in folds:\n",
    "            # 1) fit pre na TRAIN, transformuj\n",
    "            pre_fitted = pre.fit(df.loc[tr_idx, :])\n",
    "            Xtr_enc = pre_fitted.transform(df.loc[tr_idx, :])\n",
    "            Xva_enc = pre_fitted.transform(df.loc[va_idx, :])\n",
    "            feat_names = pre_fitted.get_feature_names_out()\n",
    "\n",
    "            ytr = y.loc[tr_idx]\n",
    "            yva = y.loc[va_idx]\n",
    "\n",
    "            # 2) imbalance weight (neg/pos) liczymy na TRAIN\n",
    "            pos = int((ytr == 1).sum()); neg = int((ytr == 0).sum())\n",
    "            scale_pos_weight = (neg / max(pos, 1))\n",
    "\n",
    "            # 3) zbuduj model\n",
    "            if model_name == \"xgb\":\n",
    "                clf = XGBClassifier(\n",
    "                    objective=\"binary:logistic\",\n",
    "                    eval_metric=\"auc\",\n",
    "                    tree_method=\"hist\",\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    enable_categorical=False,\n",
    "                    n_jobs=-1,\n",
    "                    **params,\n",
    "                    scale_pos_weight=scale_pos_weight\n",
    "                )\n",
    "                clf.fit(\n",
    "                    Xtr_enc, ytr,\n",
    "                    eval_set=[(Xva_enc, yva)],\n",
    "                    verbose=False,\n",
    "                    early_stopping_rounds=100\n",
    "                )\n",
    "                proba = clf.predict_proba(Xva_enc)[:, 1]\n",
    "\n",
    "            elif model_name == \"lgb\":\n",
    "                clf = LGBMClassifier(\n",
    "                    objective=\"binary\",\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    n_jobs=-1,\n",
    "                    **params\n",
    "                )\n",
    "                # dla LGBM: imbalance – można użyć is_unbalance lub scale_pos_weight\n",
    "                # (nie oba naraz). Wybierzemy scale_pos_weight dla spójności:\n",
    "                import lightgbm as lgb\n",
    "\n",
    "                lgb_callbacks = [\n",
    "                    lgb.early_stopping(stopping_rounds=200, verbose=False),  # zamiast early_stopping_rounds=\n",
    "                    lgb.log_evaluation(period=0),                            # 0 = brak logów; np. 50 aby logować co 50 iteracji\n",
    "                ]\n",
    "\n",
    "                clf.fit(\n",
    "                    Xtr_enc, ytr,\n",
    "                    eval_set=[(Xva_enc, yva)],\n",
    "                    eval_metric=\"auc\",\n",
    "                    callbacks=lgb_callbacks\n",
    "                )\n",
    "                proba = clf.predict_proba(Xva_enc)[:, 1]\n",
    "            else:\n",
    "                raise ValueError(\"model_name musi być 'xgb' lub 'lgb'.\")\n",
    "\n",
    "            auc = roc_auc_score(yva, proba)\n",
    "            fold_aucs.append(auc)\n",
    "            last = dict(pre=pre_fitted, Xva_enc=Xva_enc, yva=yva, proba=proba,\n",
    "                        feat_names=feat_names, model=clf)\n",
    "\n",
    "        mean_auc = float(np.mean(fold_aucs))\n",
    "        res = {\"model\": model_name, **params, \"AUC_mean\": mean_auc}\n",
    "        results_grid.append(res)\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc, best_cfg = mean_auc, params\n",
    "\n",
    "    grid_df = pd.DataFrame(results_grid).sort_values(\"AUC_mean\", ascending=False)\n",
    "    return grid_df, best_cfg, best_auc, last\n",
    "\n",
    "# Uruchom XGB i LGB\n",
    "xgb_grid_df, xgb_best, xgb_best_auc, xgb_last = run_model(\"xgb\", xgb_grid)\n",
    "lgb_grid_df, lgb_best, lgb_best_auc, lgb_last = run_model(\"lgb\", lgb_grid)\n",
    "\n",
    "xgb_grid_df.to_csv(f\"{ART}/cv_grid_xgb.csv\", index=False)\n",
    "lgb_grid_df.to_csv(f\"{ART}/cv_grid_lgb.csv\", index=False)\n",
    "print(\"XGB best:\", xgb_best, \"AUC_mean=\", round(xgb_best_auc, 4))\n",
    "print(\"LGB best:\", lgb_best, \"AUC_mean=\", round(lgb_best_auc, 4))\n",
    "\n",
    "# ---------- 6) Pełny zestaw metryk na foldach dla najlepszego wariantu ----------\n",
    "def eval_best(model_name, best_params):\n",
    "    metrics, last = [], {}\n",
    "    for tr_idx, va_idx in folds:\n",
    "        pre_fitted = pre.fit(df.loc[tr_idx, :])\n",
    "        Xtr_enc = pre_fitted.transform(df.loc[tr_idx, :])\n",
    "        Xva_enc = pre_fitted.transform(df.loc[va_idx, :])\n",
    "        feat_names = pre_fitted.get_feature_names_out()\n",
    "        ytr, yva = y.loc[tr_idx], y.loc[va_idx]\n",
    "        pos, neg = int((ytr==1).sum()), int((ytr==0).sum())\n",
    "        scale_pos_weight = (neg / max(pos,1))\n",
    "\n",
    "        if model_name == \"xgb\":\n",
    "            clf = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"auc\",\n",
    "                tree_method=\"hist\",\n",
    "                random_state=RANDOM_STATE,\n",
    "                enable_categorical=False,\n",
    "                n_jobs=-1,\n",
    "                **best_params,\n",
    "                scale_pos_weight=scale_pos_weight\n",
    "            )\n",
    "            clf.fit(Xtr_enc, ytr, eval_set=[(Xva_enc, yva)], verbose=False, early_stopping_rounds=100)\n",
    "        else:\n",
    "            clf = LGBMClassifier(\n",
    "                objective=\"binary\",\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                **best_params,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            clf.fit(Xtr_enc, ytr, eval_set=[(Xva_enc, yva)], eval_metric=\"auc\", callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "        p = clf.predict_proba(Xva_enc)[:,1]\n",
    "        metrics.append({\n",
    "            \"AUC\": roc_auc_score(yva, p),\n",
    "            \"PR_AUC\": average_precision_score(yva, p),\n",
    "            \"KS\": ks_score(yva, p),\n",
    "            \"Brier\": brier_score_loss(yva, p),\n",
    "            \"LogLoss\": log_loss(yva, p, labels=[0,1]),\n",
    "            \"ECE\": ece_score(yva, p)\n",
    "        })\n",
    "        last = {\"pre\":pre_fitted, \"Xva_enc\":Xva_enc, \"yva\":yva, \"pva\":p, \"feat_names\":feat_names, \"model\":clf}\n",
    "    return pd.DataFrame(metrics), last\n",
    "\n",
    "xgb_cv, xgb_last = eval_best(\"xgb\", xgb_best)\n",
    "lgb_cv, lgb_last = eval_best(\"lgb\", lgb_best)\n",
    "\n",
    "xgb_cv.to_csv(f\"{ART}/cv_fold_metrics_xgb.csv\", index=False)\n",
    "lgb_cv.to_csv(f\"{ART}/cv_fold_metrics_lgb.csv\", index=False)\n",
    "xgb_cv.mean().to_csv(f\"{ART}/cv_metrics_mean_xgb.csv\", header=False)\n",
    "lgb_cv.mean().to_csv(f\"{ART}/cv_metrics_mean_lgb.csv\", header=False)\n",
    "print(\"Średnie metryki CV XGB:\\n\", xgb_cv.mean().round(4))\n",
    "print(\"Średnie metryki CV LGB:\\n\", lgb_cv.mean().round(4))\n",
    "\n",
    "# ---------- 7) ROC i kalibracja (ostatni fold) ----------\n",
    "def plot_roc_calib(last_obj, prefix):\n",
    "    fpr, tpr, _ = roc_curve(last_obj[\"yva\"], last_obj[\"pva\"])\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(last_obj['yva'], last_obj['pva']):.3f}\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix} (ostatni fold)\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_last_fold_{prefix}.png\", dpi=160); plt.close()\n",
    "\n",
    "    frac_pos, mean_pred = calibration_curve(last_obj[\"yva\"], last_obj[\"pva\"], n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "    plt.title(f\"Kalibracja — {prefix} (ostatni fold)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{ART}/calibration_last_fold_{prefix}.png\", dpi=160); plt.close()\n",
    "\n",
    "plot_roc_calib(xgb_last, \"xgb\")\n",
    "plot_roc_calib(lgb_last, \"lgb\")\n",
    "\n",
    "# ---------- 8) Krzywa zysku + próg (ostatni fold) ----------\n",
    "def save_profit(last_obj, prefix):\n",
    "    taus, ev = profit_curve(last_obj[\"yva\"], last_obj[\"pva\"], PROFIT_GOOD, LOSS_BAD, steps=201)\n",
    "    best_tau = float(taus[int(ev.argmax())])\n",
    "    pd.DataFrame({\"tau\":taus, \"expected_profit\":ev}).to_csv(f\"{ART}/profit_curve_last_fold_{prefix}.csv\", index=False)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(taus, ev); plt.axvline(best_tau, ls=\"--\", label=f\"tau*={best_tau:.3f}\")\n",
    "    plt.xlabel(\"Próg akceptacji (p < tau)\"); plt.ylabel(\"Oczekiwany zysk\")\n",
    "    plt.title(f\"Krzywa zysku — {prefix} (ostatni fold)\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/profit_curve_last_fold_{prefix}.png\", dpi=160); plt.close()\n",
    "    return best_tau\n",
    "\n",
    "xgb_tau = save_profit(xgb_last, \"xgb\")\n",
    "lgb_tau = save_profit(lgb_last, \"lgb\")\n",
    "\n",
    "# ---------- 9) Test OOT (ostatni miesiąc) + kalibracja isotonic ----------\n",
    "if \"issue_d\" in df.columns and df[\"issue_d\"].notna().any():\n",
    "    months = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n",
    "    uniq = np.array(sorted(months.dropna().unique()))\n",
    "    oot_mask = (months == uniq[-1])\n",
    "    train_mask = ~oot_mask\n",
    "else:\n",
    "    idx = df.index.to_numpy()\n",
    "    cut = int(len(idx)*0.8)\n",
    "    train_mask = np.zeros(len(idx), dtype=bool); train_mask[:cut] = True\n",
    "    oot_mask = ~train_mask\n",
    "\n",
    "def oot_eval(best_params, model_name, tau_from_valid, last_obj, tag):\n",
    "    # fit pre na TRAIN i transformuj\n",
    "    pre_fitted = pre.fit(df.loc[train_mask, :])\n",
    "    X_train = pre_fitted.transform(df.loc[train_mask, :])\n",
    "    X_oot   = pre_fitted.transform(df.loc[oot_mask,   :])\n",
    "    y_train = y.loc[train_mask]\n",
    "    y_oot_  = y.loc[oot_mask]\n",
    "    feat_names = pre_fitted.get_feature_names_out()\n",
    "\n",
    "    pos, neg = int((y_train==1).sum()), int((y_train==0).sum())\n",
    "    scale_pos_weight = (neg / max(pos,1))\n",
    "\n",
    "    if model_name == \"xgb\":\n",
    "        clf = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            enable_categorical=False,\n",
    "            n_jobs=-1,\n",
    "            **best_params,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "        clf.fit(X_train, y_train, eval_set=[(X_train, y_train)], verbose=False)\n",
    "    else:\n",
    "        clf = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            **best_params,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "        clf.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=\"auc\", callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "    # kalibracja na walidacji z ostatniego foldu\n",
    "    calibrated = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"isotonic\")\n",
    "    calibrated.fit(last_obj[\"Xva_enc\"], last_obj[\"yva\"])\n",
    "    p_oot = calibrated.predict_proba(X_oot)[:,1]\n",
    "\n",
    "    # metryki OOT\n",
    "    oot_metrics = {\n",
    "        \"AUC\": roc_auc_score(y_oot_, p_oot),\n",
    "        \"PR_AUC\": average_precision_score(y_oot_, p_oot),\n",
    "        \"KS\": ks_score(y_oot_, p_oot),\n",
    "        \"Brier\": brier_score_loss(y_oot_, p_oot),\n",
    "        \"LogLoss\": log_loss(y_oot_, p_oot, labels=[0,1]),\n",
    "        \"ECE\": ece_score(y_oot_, p_oot)\n",
    "    }\n",
    "    pd.Series(oot_metrics).to_csv(f\"{ART}/oot_metrics_{tag}.csv\", header=False)\n",
    "    print(f\"\\nMetryki OOT ({tag}):\\n\", pd.Series(oot_metrics).round(4))\n",
    "\n",
    "    # ROC/kalibracja\n",
    "    fpr, tpr, _ = roc_curve(y_oot_, p_oot)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_oot_,p_oot):.3f}\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {tag} (OOT)\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(f\"{ART}/roc_oot_{tag}.png\", dpi=160); plt.close()\n",
    "\n",
    "    frac_pos, mean_pred = calibration_curve(y_oot_, p_oot, n_bins=N_BINS_CALIB, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"Przewidziana PD\"); plt.ylabel(\"Zaobserwowana stopa defaultu\")\n",
    "    plt.title(f\"Kalibracja — {tag} (OOT)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{ART}/calibration_oot_{tag}.png\", dpi=160); plt.close()\n",
    "\n",
    "    # decyle i KS\n",
    "    dec_tab = decile_table(y_oot_, p_oot, deciles=10)\n",
    "    dec_tab.to_csv(f\"{ART}/decile_table_oot_{tag}.csv\", index=False)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(dec_tab[\"decile\"], dec_tab[\"ks\"], marker=\"o\")\n",
    "    plt.xlabel(\"Decyl (1 = najwyższe ryzyko)\"); plt.ylabel(\"KS\")\n",
    "    plt.title(f\"KS po decylach — {tag} (OOT)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{ART}/ks_by_decile_oot_{tag}.png\", dpi=160); plt.close()\n",
    "\n",
    "    # decyzje wg progu z walidacji\n",
    "    accept_oot = (p_oot < tau_from_valid)\n",
    "    tg = int(((y_oot_==0) & accept_oot).sum())\n",
    "    tb = int(((y_oot_==1) & accept_oot).sum())\n",
    "    ev_oot = tg*PROFIT_GOOD + tb*LOSS_BAD\n",
    "    pd.Series({\n",
    "        \"best_tau_from_valid\": tau_from_valid,\n",
    "        \"accepted_cnt\": int(accept_oot.sum()),\n",
    "        \"true_good_accepted\": tg,\n",
    "        \"true_bad_accepted\": tb,\n",
    "        \"expected_profit_OOT\": ev_oot\n",
    "    }).to_csv(f\"{ART}/decision_summary_oot_{tag}.csv\", header=False)\n",
    "\n",
    "    # ważności (gain/split → zamieniamy na barplot TOP-15)\n",
    "    try:\n",
    "        importances = getattr(clf, \"feature_importances_\", None)\n",
    "        if importances is not None and len(importances) == len(feat_names):\n",
    "            imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "            imp_df.to_csv(f\"{ART}/feature_importance_{tag}.csv\", index=False)\n",
    "            plt.figure(figsize=(8,6))\n",
    "            top = imp_df.head(15)[::-1]\n",
    "            plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "            plt.title(f\"{tag} — TOP 15 ważności cech\")\n",
    "            plt.tight_layout(); plt.savefig(f\"{ART}/feature_importance_top15_{tag}.png\", dpi=160); plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return clf, pre_fitted, feat_names\n",
    "\n",
    "# XGB – OOT\n",
    "xgb_clf, xgb_pre, xgb_feats = oot_eval(xgb_best, \"xgb\", xgb_tau, xgb_last, \"xgb\")\n",
    "# LGB – OOT\n",
    "lgb_clf, lgb_pre, lgb_feats = oot_eval(lgb_best, \"lgb\", lgb_tau, lgb_last, \"lgb\")\n",
    "\n",
    "print(f\"\\nArtefakty zapisano w: {os.path.abspath(ART)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
